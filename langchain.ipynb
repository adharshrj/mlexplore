{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adharshrj/mlexplore/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b11b26db-2ea6-4280-a648-52d0c5d96e7b",
      "metadata": {
        "id": "b11b26db-2ea6-4280-a648-52d0c5d96e7b"
      },
      "source": [
        "# Step 1 : Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd387dc-2113-43f7-b279-ccafbe14fda1",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dd387dc-2113-43f7-b279-ccafbe14fda1",
        "outputId": "7be4c938-8ca0-4936-ce7f-7904e49bc78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.202-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.9 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.10-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.202 langchainplus-sdk-0.0.10 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.15.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=27260c865cb8c21087207d04100fb37b3c7d1a469845a59753bee465eae45eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, transformers, sentence_transformers\n",
            "Successfully installed safetensors-0.3.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.7.6-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argilla (from unstructured)\n",
            "  Downloading argilla-1.10.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.0.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.2)\n",
            "Collecting msg-parser (from unstructured)\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.5.3)\n",
            "Collecting pdf2image (from unstructured)\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pdfminer.six (from unstructured)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unstructured) (8.4.0)\n",
            "Collecting pypandoc (from unstructured)\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Collecting python-docx (from unstructured)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-pptx (from unstructured)\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.8.10)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.0.1)\n",
            "Collecting httpx<0.24,>=0.15 (from argilla->unstructured)\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated~=1.2.0 (from argilla->unstructured)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (23.1)\n",
            "Requirement already satisfied: pydantic>=1.10.7 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (1.10.7)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.13 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (1.14.1)\n",
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (4.65.0)\n",
            "Collecting backoff (from argilla->unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting monotonic (from argilla->unstructured)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting rich<=13.0.1 (from argilla->unstructured)\n",
            "  Downloading rich-13.0.1-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<1.0.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2022.7.1)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2022.10.31)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured) (40.0.2)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured)\n",
            "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
            "Collecting httpcore<0.17.0,>=0.15.0 (from httpx<0.24,>=0.15->argilla->unstructured)\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3 (from httpx<0.24,>=0.15->argilla->unstructured)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.7->argilla->unstructured) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<=13.0.1->argilla->unstructured)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<=13.0.1->argilla->unstructured) (2.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (3.6.2)\n",
            "Building wheels for collected packages: python-docx, python-pptx, olefile\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=20b60dee7f8fc9f9d2c87d053fae1871d8dfe4257d30b612cc7faa0e7ab9c7cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470935 sha256=c9e54ff6d527f87db009b45fbfcc27b5d02f156f6650fb49de7f6280f6a56c3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=b7c2256924695d51bf6a2b238a8c4d7c37b505b39e7a24c56f117ff013b4201b\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "Successfully built python-docx python-pptx olefile\n",
            "Installing collected packages: rfc3986, monotonic, filetype, commonmark, XlsxWriter, rich, python-magic, python-docx, pypandoc, pdf2image, olefile, h11, deprecated, backoff, python-pptx, msg-parser, httpcore, pdfminer.six, httpx, argilla, unstructured\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.4\n",
            "    Uninstalling rich-13.3.4:\n",
            "      Successfully uninstalled rich-13.3.4\n",
            "Successfully installed XlsxWriter-3.1.2 argilla-1.10.0 backoff-2.2.1 commonmark-0.9.1 deprecated-1.2.14 filetype-1.2.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 monotonic-1.6 msg-parser-1.2.0 olefile-0.46 pdf2image-1.16.3 pdfminer.six-20221105 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 rfc3986-1.5.0 rich-13.0.1 unstructured-0.7.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.1\n"
          ]
        }
      ],
      "source": [
        "# Pip installation LangChain and Hugginface API\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Pip installation of additional needed libraries\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured\n",
        "\n",
        "# To download the transcript of a youtube video\n",
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3409a776-5bb6-4f40-9751-d316649c89ae",
      "metadata": {
        "id": "3409a776-5bb6-4f40-9751-d316649c89ae"
      },
      "source": [
        "# Step 2 : Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed8cfad-8e90-436e-a2d3-49b675363129",
      "metadata": {
        "id": "6ed8cfad-8e90-436e-a2d3-49b675363129"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b05718-d806-496e-88c2-4303d2ebff63",
      "metadata": {
        "id": "87b05718-d806-496e-88c2-4303d2ebff63"
      },
      "source": [
        "# Step 3 : Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbLpkvOhzXeu",
        "outputId": "b4471e85-4bcf-443d-ddaa-a9e84dc5bfbe"
      },
      "id": "qbLpkvOhzXeu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0879212a-ab69-4df6-b124-fab0d6a72ce2",
      "metadata": {
        "id": "0879212a-ab69-4df6-b124-fab0d6a72ce2"
      },
      "source": [
        "# Step 4 : Load files as test using TextLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Load Text files using TextLoader"
      ],
      "metadata": {
        "id": "Mq36hk_91HFq"
      },
      "id": "Mq36hk_91HFq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945f07ea-7b4a-4abc-9618-5ec798e94e74",
      "metadata": {
        "id": "945f07ea-7b4a-4abc-9618-5ec798e94e74"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1 Load text from mounted google drive file"
      ],
      "metadata": {
        "id": "2xtvbirHltJt"
      },
      "id": "2xtvbirHltJt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20ea47c-5005-435c-9f18-828a29080b22",
      "metadata": {
        "id": "a20ea47c-5005-435c-9f18-828a29080b22"
      },
      "outputs": [],
      "source": [
        "def loadTXTFileFromLocal(local_file_name=\"/content/drive/MyDrive/LLM_Testing_Docs/local_text_file.txt\"):\n",
        "    # Load the text data\n",
        "    with open(local_file_name, \"r\", encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    with open(local_file_name, \"w\",  encoding='utf-8') as file:\n",
        "      file.write(text)\n",
        "\n",
        "    # Load the text document using TextLoader\n",
        "    loader = TextLoader(local_file_name)\n",
        "    loaded_docs = loader.load()\n",
        "    return loaded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cccecbc6-8bca-4c0b-b75c-2a63b1caf359",
      "metadata": {
        "id": "cccecbc6-8bca-4c0b-b75c-2a63b1caf359"
      },
      "source": [
        "#### 4.1.2 Load text from URL endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915dd66f-75bb-4c77-b26a-74773465a200",
      "metadata": {
        "id": "915dd66f-75bb-4c77-b26a-74773465a200"
      },
      "outputs": [],
      "source": [
        "def loadTXTFileFromURL(text_file_url=\"https://raw.githubusercontent.com/adharshrj/currency-in-wordsinr/main/README.md\"):\n",
        "    # Fetching the text file\n",
        "    output_file_name = \"url_text_file.txt\"\n",
        "    response = requests.get(text_file_url)\n",
        "    with open(output_file_name, \"w\",  encoding='utf-8') as file:\n",
        "      file.write(response.text)\n",
        "\n",
        "    # Load the text document using TextLoader\n",
        "    loader = TextLoader('./'+output_file_name)\n",
        "    loaded_docs = loader.load()\n",
        "    return loaded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090cedd5-54ce-4750-93d9-4e5482d4a33d",
      "metadata": {
        "id": "090cedd5-54ce-4750-93d9-4e5482d4a33d"
      },
      "source": [
        "### 4.2 Load PDF form Mounted Google Drive using PDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8eb936a-e1fd-47e7-82be-c51964ef5c8c",
      "metadata": {
        "id": "d8eb936a-e1fd-47e7-82be-c51964ef5c8c"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5805a745-50fb-4513-b66e-49573649ce4b",
      "metadata": {
        "id": "5805a745-50fb-4513-b66e-49573649ce4b"
      },
      "outputs": [],
      "source": [
        "def loadPDFFromLocal(pdf_file_path=\"/content/drive/MyDrive/LLM_Testing_Docs/Eurovision_Song_Contest_2023.pdf\"):\n",
        "    loader = UnstructuredPDFLoader(pdf_file_path)\n",
        "    loaded_docs = loader.load()\n",
        "    return loaded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28397ce0-d1ef-49d0-85fd-918665439267",
      "metadata": {
        "id": "28397ce0-d1ef-49d0-85fd-918665439267"
      },
      "source": [
        "### 4.3 Load Data from a Website using URLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da07186d-e29c-4890-870f-fe0109d0119f",
      "metadata": {
        "id": "da07186d-e29c-4890-870f-fe0109d0119f"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed8976b4-4e69-4f55-b3eb-c3db943b3d9d",
      "metadata": {
        "id": "ed8976b4-4e69-4f55-b3eb-c3db943b3d9d"
      },
      "outputs": [],
      "source": [
        "def loadTextFromWebsite(url=\"https://saturncloud.io/blog/breaking-the-data-barrier-how-zero-shot-one-shot-and-few-shot-learning-are-transforming-machine-learning/\"):\n",
        "    loader = UnstructuredURLLoader(urls=[url])\n",
        "    loaded_docs = loader.load()\n",
        "    return loaded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07c15cf-1574-4441-9391-0f26b8ede1e2",
      "metadata": {
        "id": "e07c15cf-1574-4441-9391-0f26b8ede1e2"
      },
      "source": [
        "### 4.4 Load Video Transcriptions (Youtube video) using YouTubeTranscriptApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab058904-b69f-4916-9d84-7879c6007049",
      "metadata": {
        "id": "ab058904-b69f-4916-9d84-7879c6007049"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922e4082-c2a3-4883-8a49-d27767bbc54a",
      "metadata": {
        "id": "922e4082-c2a3-4883-8a49-d27767bbc54a"
      },
      "outputs": [],
      "source": [
        "def loadTextFromYoutubeVideo(youtube_video_id=\"eg9qDjws_bU\"):\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n",
        "\n",
        "    transcript_text = \"\"\n",
        "    for entry in transcript:\n",
        "        transcript_text += ' ' + entry['text']\n",
        "\n",
        "    youtube_local_txt_file = \"youtube_transcript.txt\"\n",
        "    with open('./'+youtube_local_txt_file, \"w\",  encoding='utf-8') as file:\n",
        "      file.write(transcript_text)\n",
        "\n",
        "    # Load the text document using TextLoader\n",
        "    loader = TextLoader('./'+youtube_local_txt_file)\n",
        "    loaded_docs = loader.load()\n",
        "    return loaded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713c7310-c185-42ba-8c69-de4a55ebdd20",
      "metadata": {
        "id": "713c7310-c185-42ba-8c69-de4a55ebdd20"
      },
      "source": [
        "# Step 5 : Split the documents in chunks (**Important** : LLMs cannot accept long inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49ef93a-b00c-4466-8e41-21ea27504079",
      "metadata": {
        "id": "c49ef93a-b00c-4466-8e41-21ea27504079"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9aef4a-9bb4-49e4-97ce-9eb3ea356290",
      "metadata": {
        "id": "aa9aef4a-9bb4-49e4-97ce-9eb3ea356290"
      },
      "outputs": [],
      "source": [
        "def splitDocument(loaded_docs):\n",
        "    # Splitting documents into chunks\n",
        "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "    chunked_docs = splitter.split_documents(loaded_docs)\n",
        "    return chunked_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f333094-c0bb-4117-910c-fa3dc16f7eb7",
      "metadata": {
        "id": "2f333094-c0bb-4117-910c-fa3dc16f7eb7"
      },
      "source": [
        "# Step 6 : Convert the documents into embeddings and store them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "781fc65f-5e72-4842-b30d-c74c5b806f26",
      "metadata": {
        "id": "781fc65f-5e72-4842-b30d-c74c5b806f26"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd057fc0-70b0-4ef6-8018-6136fe6c8dd7",
      "metadata": {
        "id": "bd057fc0-70b0-4ef6-8018-6136fe6c8dd7"
      },
      "outputs": [],
      "source": [
        "def createEmbeddings(chunked_docs):\n",
        "    # Create embeddings and store them in a FAISS vector store\n",
        "    embedder = HuggingFaceEmbeddings()\n",
        "    vector_store = FAISS.from_documents(chunked_docs, embedder)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37754d9c-ee8c-4d08-82b7-846202e75cba",
      "metadata": {
        "id": "37754d9c-ee8c-4d08-82b7-846202e75cba"
      },
      "source": [
        "# Step 7 : Use those embeddings to feed the LLM model and Answer Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560479d2-d656-46cc-b672-9393d3d108be",
      "metadata": {
        "id": "560479d2-d656-46cc-b672-9393d3d108be"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e492b83-9d1e-48dc-abc2-86638f1f9a96",
      "metadata": {
        "id": "1e492b83-9d1e-48dc-abc2-86638f1f9a96"
      },
      "outputs": [],
      "source": [
        "def loadLLMModel():\n",
        "    llm=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "    return chain\n",
        "\n",
        "def askQuestions(vector_store, chain, question):\n",
        "    # Ask a question using the QA chain\n",
        "    similar_docs = vector_store.similarity_search(question)\n",
        "    response = chain.run(input_documents=similar_docs, question=question)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861d5dc9-3345-41b5-ab89-fa5110a84a3b",
      "metadata": {
        "id": "861d5dc9-3345-41b5-ab89-fa5110a84a3b"
      },
      "outputs": [],
      "source": [
        "chain = loadLLMModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21adbcda-48d7-4404-9812-08aa7985fc0a",
      "metadata": {
        "id": "21adbcda-48d7-4404-9812-08aa7985fc0a"
      },
      "source": [
        "# Step 8 : Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Test with Local file & Test with file from URL"
      ],
      "metadata": {
        "id": "e9Iw27sInLP8"
      },
      "id": "e9Iw27sInLP8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05458723-fd52-4901-90b3-4be88efc7f0d",
      "metadata": {
        "id": "05458723-fd52-4901-90b3-4be88efc7f0d"
      },
      "outputs": [],
      "source": [
        "LOCAL_loaded_docs = loadTXTFileFromLocal()\n",
        "LOCAL_chunked_docs = splitDocument(LOCAL_loaded_docs)\n",
        "LOCAL_vector_store = createEmbeddings(LOCAL_chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf93661-aafd-40b6-913c-7fc273fa22f3",
      "metadata": {
        "id": "1cf93661-aafd-40b6-913c-7fc273fa22f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3372c5e-9dad-4c95-c749-f0e23726bb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT and plugins are helping Citizen Data Scientists by providing them with the tools they need to analyze and interpret data. By enabling them to use natural language, they are able to ask questions and get answers in plain English, without knowing complex programming languages or statistical techniques. Additionally, ChatGPT is a personal expert who is always available to help them turn their idea into reality.\n"
          ]
        }
      ],
      "source": [
        "LOCAL_response = askQuestions(LOCAL_vector_store, chain, \"Explain me how ChatGPT and Plugin are empowering Citizen Data Scientists?\")\n",
        "print(LOCAL_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa0eae1e-0fb4-4462-85e6-d8d963e3b12d",
      "metadata": {
        "id": "aa0eae1e-0fb4-4462-85e6-d8d963e3b12d"
      },
      "source": [
        "### 8.2 Test with file from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513281b6-9849-4776-bcb1-eae6cdcc4a14",
      "metadata": {
        "id": "513281b6-9849-4776-bcb1-eae6cdcc4a14"
      },
      "outputs": [],
      "source": [
        "URL_loaded_docs = loadTXTFileFromURL()\n",
        "URL_chunked_docs = splitDocument(URL_loaded_docs)\n",
        "URL_vector_store = createEmbeddings(URL_chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9f68b3-4b26-463f-90de-3615445cf7c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9f68b3-4b26-463f-90de-3615445cf7c0",
        "outputId": "2187cd2b-396a-440e-f9e6-0d3e73d42cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import the library using the import or require approach.\n"
          ]
        }
      ],
      "source": [
        "URL_response = askQuestions(URL_vector_store, chain, \"How do I import?\")\n",
        "print(URL_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c417bb78-9288-4b4b-9508-4ca5d3e9118d",
      "metadata": {
        "id": "c417bb78-9288-4b4b-9508-4ca5d3e9118d"
      },
      "source": [
        "### 8.3 Test with PDF from local path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d35796-05d7-45d8-9736-3134c78adb13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d35796-05d7-45d8-9736-3134c78adb13",
        "outputId": "a333eb01-4c71-48cb-8578-f38fcac5ec85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1019, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1316, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1425, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1352, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "PDF_loaded_docs = loadPDFFromLocal()\n",
        "PDF_chunked_docs = splitDocument(PDF_loaded_docs)\n",
        "PDF_vector_store = createEmbeddings(PDF_chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14452aec-f775-449b-b245-826025e0de71",
      "metadata": {
        "id": "14452aec-f775-449b-b245-826025e0de71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cf4cde-cfad-415b-8bb0-e88014fc4322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second semi-final took place on 11 May 2023 at 20:00 BST (21:00 CEST).\n"
          ]
        }
      ],
      "source": [
        "PDF_response = askQuestions(PDF_vector_store, chain, \"When did semi-final happen?\")\n",
        "print(PDF_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f426c1-b5fc-4348-9516-9d9923a3832d",
      "metadata": {
        "id": "80f426c1-b5fc-4348-9516-9d9923a3832d"
      },
      "source": [
        "### 8.4 Test with Website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8d9a41-b15d-4a9b-aad5-de18a7815007",
      "metadata": {
        "id": "4e8d9a41-b15d-4a9b-aad5-de18a7815007"
      },
      "outputs": [],
      "source": [
        "WEBSITE_loaded_docs = loadTextFromWebsite()\n",
        "WEBSITE_chunked_docs = splitDocument(WEBSITE_loaded_docs)\n",
        "WEBSITE_vector_store = createEmbeddings(WEBSITE_chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c10d983-72b9-4c99-a473-5fb840d23993",
      "metadata": {
        "id": "9c10d983-72b9-4c99-a473-5fb840d23993",
        "outputId": "b08fdab8-a70e-4a30-a75e-e92c848f2153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This article is about the basics of machine learning. It explains how it works, how it works, and how it can be used to make predictions. It also explains how it works and how it can be used to make decisions. It also explains how it works and how it can be used to make predictions. Finally, it explains how it can be used to make predictions and how it can be used to make decisions.\n"
          ]
        }
      ],
      "source": [
        "WEBSITE_response = askQuestions(WEBSITE_vector_store, chain, \"Summarize this in 1 page for someone aged 18 who is a high school senior\")\n",
        "print(WEBSITE_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a96c417c-cc60-4e71-ac34-ba2183c0a370",
      "metadata": {
        "id": "a96c417c-cc60-4e71-ac34-ba2183c0a370"
      },
      "source": [
        "### 8.5 Test with text from video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd000306-fc8d-48f2-9fbd-f1b0674041b9",
      "metadata": {
        "id": "fd000306-fc8d-48f2-9fbd-f1b0674041b9"
      },
      "outputs": [],
      "source": [
        "VIDEO_loaded_docs = loadTextFromYoutubeVideo()\n",
        "VIDEO_chunked_docs = splitDocument(VIDEO_loaded_docs)\n",
        "VIDEO_vector_store = createEmbeddings(VIDEO_chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d264c4fe-5033-4935-bec6-197aca180ffb",
      "metadata": {
        "id": "d264c4fe-5033-4935-bec6-197aca180ffb",
        "outputId": "d7493471-9c22-48cb-bca1-ede423b50bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The speaker believes that AI is becoming more advanced and dangerous than nuclear warheads. They believe that AI could be used to make incredibly effective propaganda and could be used to manipulate people. They believe that one day humans will be able to serve the machines and not the other way around.\n"
          ]
        }
      ],
      "source": [
        "VIDEO_response = askQuestions(VIDEO_vector_store, chain, \"Summarize the video?\")\n",
        "print(VIDEO_response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}